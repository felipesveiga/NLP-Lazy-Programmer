{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a8388f",
   "metadata": {
    "papermill": {
     "duration": 0.005756,
     "end_time": "2024-03-28T14:19:56.668316",
     "exception": false,
     "start_time": "2024-03-28T14:19:56.662560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Poe x Frost \n",
    "* This project aims to conceive a classification strategy of differentiating Edgar Allan Poe's poems from Robert Frost's. We'll conduct the experiment by training two separate Markov Models, each of them calibrated with the poems of one of the poets.\n",
    "* Then, given the $p(\\text{text}|\\text{author})$ returned by the models, we'll apply the Bayes' Theorem to compute $p(\\text{author}|\\text{text})$ to receive the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf438239",
   "metadata": {
    "papermill": {
     "duration": 0.00522,
     "end_time": "2024-03-28T14:19:56.679008",
     "exception": false,
     "start_time": "2024-03-28T14:19:56.673788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb9a36b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:19:56.692292Z",
     "iopub.status.busy": "2024-03-28T14:19:56.691589Z",
     "iopub.status.idle": "2024-03-28T14:19:56.731480Z",
     "shell.execute_reply": "2024-03-28T14:19:56.730606Z"
    },
    "papermill": {
     "duration": 0.049494,
     "end_time": "2024-03-28T14:19:56.733859",
     "exception": false,
     "start_time": "2024-03-28T14:19:56.684365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from typing import List\n",
    "\n",
    "def load_text(filename:str)->List[str]:\n",
    "    '''\n",
    "        Reads the .txt file.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        `filename`: str\n",
    "            The name of the poems file.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A list containing each strophe's content.\n",
    "    '''\n",
    "    with open(f'/kaggle/input/poe-vs-frost/{filename}', 'r') as f:\n",
    "        strophe_delim = '\\n\\n'\n",
    "        return sub('\\n\\u2009\\n', strophe_delim, f.read()).split(strophe_delim)\n",
    "    \n",
    "txt_frost = load_text('05_robert_frost.txt')\n",
    "txt_poe = load_text('05_edgar_allan_poe.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023fdd3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:19:56.746181Z",
     "iopub.status.busy": "2024-03-28T14:19:56.745788Z",
     "iopub.status.idle": "2024-03-28T14:19:56.751519Z",
     "shell.execute_reply": "2024-03-28T14:19:56.750311Z"
    },
    "papermill": {
     "duration": 0.014135,
     "end_time": "2024-03-28T14:19:56.753560",
     "exception": false,
     "start_time": "2024-03-28T14:19:56.739425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth; \n"
     ]
    }
   ],
   "source": [
    "# Note that the variables are lists of strophes content. \n",
    "print(txt_frost[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3fc56",
   "metadata": {
    "papermill": {
     "duration": 0.0053,
     "end_time": "2024-03-28T14:19:56.764215",
     "exception": false,
     "start_time": "2024-03-28T14:19:56.758915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Datasets split\n",
    "* As in any Data Science project, we'll have to split our sets in two partitions. One dedicated to training our models and the other one to simply estimate the algorithm's performance in deployment scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fd1b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:19:56.776696Z",
     "iopub.status.busy": "2024-03-28T14:19:56.776064Z",
     "iopub.status.idle": "2024-03-28T14:19:57.962185Z",
     "shell.execute_reply": "2024-03-28T14:19:57.961324Z"
    },
    "papermill": {
     "duration": 1.19502,
     "end_time": "2024-03-28T14:19:57.964661",
     "exception": false,
     "start_time": "2024-03-28T14:19:56.769641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_poe, test_poe = train_test_split(txt_poe, train_size=.75, random_state=42)\n",
    "train_frost, test_frost = train_test_split(txt_frost, train_size=.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1567da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:19:57.979493Z",
     "iopub.status.busy": "2024-03-28T14:19:57.978780Z",
     "iopub.status.idle": "2024-03-28T14:19:57.986626Z",
     "shell.execute_reply": "2024-03-28T14:19:57.985614Z"
    },
    "papermill": {
     "duration": 0.016523,
     "end_time": "2024-03-28T14:19:57.988677",
     "exception": false,
     "start_time": "2024-03-28T14:19:57.972154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6460176991150443"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we are dealing with an unbalanced dataset.\n",
    "len(txt_frost)/ (len(txt_poe)+len(txt_frost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5190341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:19:58.001271Z",
     "iopub.status.busy": "2024-03-28T14:19:58.000879Z",
     "iopub.status.idle": "2024-03-28T14:19:58.005411Z",
     "shell.execute_reply": "2024-03-28T14:19:58.004605Z"
    },
    "papermill": {
     "duration": 0.013071,
     "end_time": "2024-03-28T14:19:58.007341",
     "exception": false,
     "start_time": "2024-03-28T14:19:57.994270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's store each class' probability in a dictionary.\n",
    "proba_frost = len(txt_frost)/ (len(txt_poe)+len(txt_frost))\n",
    "probas = {'frost':proba_frost, 'poe':(1-proba_frost)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4bbed",
   "metadata": {
    "papermill": {
     "duration": 0.005629,
     "end_time": "2024-03-28T14:19:58.018771",
     "exception": false,
     "start_time": "2024-03-28T14:19:58.013142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fitting our Models\n",
    "* Let's use the class below in order to generate the $A$'s and $\\pi$'s of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15cfca04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:19:58.032509Z",
     "iopub.status.busy": "2024-03-28T14:19:58.031917Z",
     "iopub.status.idle": "2024-03-28T14:19:59.375073Z",
     "shell.execute_reply": "2024-03-28T14:19:59.374240Z"
    },
    "papermill": {
     "duration": 1.353105,
     "end_time": "2024-03-28T14:19:59.377644",
     "exception": false,
     "start_time": "2024-03-28T14:19:58.024539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Tuple\n",
    "\n",
    "class MarkovModel:\n",
    "    '''\n",
    "       Markov Model, with Add-Epsilon Smoothing.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        `corpus`: List[str]\n",
    "            List with the documents to be used.\n",
    "        `epsilon`: float\n",
    "            Smoothing degree of the probabilities.\n",
    "        `name`: str\n",
    "            A name for your model.\n",
    "            \n",
    "        Methods\n",
    "        ------\n",
    "        `fit`: Generates the model's A and pi.\n",
    "        `predict_log_proba`: Estimates the probability's log of a given sequence.\n",
    "        \n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        `a`: `pd.DataFrame`\n",
    "            The model's A matrix.\n",
    "        `_a`: Dict[str, Dict[str, int]]\n",
    "            A Dictionary mapping the number of occurences a given state transition happened.\n",
    "        `pi`: `pd.Series`\n",
    "            The model's pi vector.\n",
    "        `_pi`: Dict[str, int]\n",
    "            A dictionary informing the amount of times a given token started a sentence.\n",
    "        `_vocab`: Set[str]\n",
    "            A set object with all the corpus's vocabulary.\n",
    "    '''\n",
    "    def __init__(self, corpus:List[str], epsilon:float, name:str):\n",
    "        self.corpus = self.split_corpus(corpus)\n",
    "        self.corpus_length = len(self.corpus)\n",
    "        self.epsilon = epsilon\n",
    "        self.name = name\n",
    "\n",
    "    @staticmethod\n",
    "    def split_corpus(corpus:List[str])->List[List[str]]:\n",
    "        '''\n",
    "            Tokenizes the corpus' documents.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `corpus`: List[str]\n",
    "                A list with each of the corpus' documents.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            A list of the documents tokens.\n",
    "        '''\n",
    "        return [word_tokenize(document.lower()) for document in corpus]\n",
    "    \n",
    "    def __vocab(self)->None:\n",
    "        '''\n",
    "            Extraction of all the corpus tokens.\n",
    "            \n",
    "            We create a set with all training tokens and another one disregarding the ones only used as first word of the strophes.\n",
    "        '''\n",
    "        self._vocab, self._a_vocab = [], []\n",
    "        \n",
    "        for doc in self.corpus:\n",
    "            self._vocab += doc\n",
    "            self._a_vocab+=doc[1:] # Not including the first tokens.\n",
    "            \n",
    "        self._vocab, self._a_vocab = set(self._vocab), set(self._a_vocab)        \n",
    "    \n",
    "    def __check_pi(self, token:str)->str:\n",
    "        '''\n",
    "            Masks a sentence's first token with '<UNKNOWN>' mark if it is not included in the training set.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `token`: str\n",
    "                The sentence's first token under scrutiny\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The treated token.\n",
    "        '''\n",
    "        return token if token in self._pi else '<UNKNOWN>'\n",
    "    \n",
    "    def __check_a(self, token1:str, token2:str)->Tuple[str]:\n",
    "        '''\n",
    "            When querying the model's A matrix, checks whether the provided initial and target states are present. If not,\n",
    "            the tokens are masked with the flag '<UNKNOWN>'.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            `token1`: str\n",
    "                The initial state.\n",
    "            `token2`: str\n",
    "                The target state.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The treated tokens inside a tuple.\n",
    "        '''\n",
    "        token1 = token1 if token1 in self.a.index else '<UNKNOWN>'\n",
    "        token2 = token2 if token2 in self.a.columns else '<UNKNOWN>'\n",
    "        return token1, token2\n",
    "    \n",
    "    def __pi(self):\n",
    "        '''\n",
    "            Encharged for measuring the model's pi vector.\n",
    "        '''\n",
    "        self._pi = {}\n",
    "        m = self.a.shape[0]\n",
    "        \n",
    "        for doc in self.corpus:\n",
    "            i = doc[0]\n",
    "            if i not in self._pi.keys():\n",
    "                self._pi[i] = 1\n",
    "            else:\n",
    "                self._pi[i]+=1\n",
    "        \n",
    "        self._pi['<UNKNOWN>'] = 0 # Defining a key for possible tokens of the test set that were unseen during training.\n",
    "        self.pi =  (pd.Series(self._pi)+self.epsilon) / (self.corpus_length+self.epsilon*m)\n",
    "        \n",
    "    def __a(self):\n",
    "        '''\n",
    "            Measures the model's A matrix.\n",
    "        '''\n",
    "        self._a = {j:{} for j in self._a_vocab}\n",
    "        for doc in self.corpus:\n",
    "            for idx, j in enumerate(doc[1:], start=1):\n",
    "                d_j = self._a[j]\n",
    "                i = doc[idx-1]\n",
    "                if i not in d_j.keys():\n",
    "                    d_j[i] = 1\n",
    "                else:\n",
    "                    d_j[i] += 1\n",
    "        self._a['<UNKNOWN>'] = {'<UNKNOWN>':0}\n",
    "        a = pd.DataFrame(self._a).fillna(0)\n",
    "        num = (a+self.epsilon)\n",
    "        denom = a.sum(axis=1, skipna=True)+a.shape[0]*self.epsilon\n",
    "        self.a =  num.div(denom, axis=0) \n",
    "        \n",
    "\n",
    "    def fit(self):\n",
    "        '''\n",
    "            Fits the algorithm to the provided corpus.\n",
    "        '''\n",
    "        self.__vocab()\n",
    "        self.__a()\n",
    "        self.__pi()\n",
    "        return self\n",
    "    \n",
    "    def predict_log_proba(self, text:str)->float:\n",
    "        '''\n",
    "            Estimates the probability's log of a given sequence.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `text`: str\n",
    "                The text whose probability needs to be computed.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The sequence's log probability.\n",
    "        '''\n",
    "        text = word_tokenize(text.lower())\n",
    "        proba_pi = np.log(self.pi[self.__check_pi(text[0])])\n",
    "        proba_a = np.log([self.a.loc[self.__check_a(text[i], text[i+1])] for i, _ in enumerate(text[:-1])]) \n",
    "        return proba_pi + np.sum(proba_a)                                                   \n",
    "    \n",
    "    def predict_proba(self, text:str)->float:\n",
    "        '''\n",
    "            Estimates the probability of a given sequence.\n",
    "            \n",
    "            *Note:* There is a risk of the output to be 0 for long sequences.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `text`: str\n",
    "                The text whose probability needs to be computed.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The sequence's probability.\n",
    "        '''\n",
    "        return np.exp(self.predict_log_proba(text))\n",
    "    \n",
    "    def predict_log_proba_author(self, text:str)->float:\n",
    "        '''\n",
    "            Measures the likelihood that a given text was written by the model's author by Bayes' Theorem.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `text`: str\n",
    "                The text under scrutiny.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The computed probability.\n",
    "        '''\n",
    "        global probas\n",
    "        return self.predict_log_proba(text) + np.log(probas[self.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad66905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:19:59.391228Z",
     "iopub.status.busy": "2024-03-28T14:19:59.390692Z",
     "iopub.status.idle": "2024-03-28T14:20:00.571086Z",
     "shell.execute_reply": "2024-03-28T14:20:00.570174Z"
    },
    "papermill": {
     "duration": 1.189882,
     "end_time": "2024-03-28T14:20:00.573560",
     "exception": false,
     "start_time": "2024-03-28T14:19:59.383678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally fitting our Markov Models.\n",
    "model_frost = MarkovModel(train_frost, 1, 'frost').fit()\n",
    "model_poe = MarkovModel(train_poe, 1, 'poe').fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c712c9",
   "metadata": {
    "papermill": {
     "duration": 0.00578,
     "end_time": "2024-03-28T14:20:00.585543",
     "exception": false,
     "start_time": "2024-03-28T14:20:00.579763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Stage\n",
    "* Now, we are able to use our fitted models in order to predict the probability a given stranza was written by Edgar Allan Poe or Robert Frost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef21748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:20:00.600633Z",
     "iopub.status.busy": "2024-03-28T14:20:00.600280Z",
     "iopub.status.idle": "2024-03-28T14:20:00.609279Z",
     "shell.execute_reply": "2024-03-28T14:20:00.608182Z"
    },
    "papermill": {
     "duration": 0.018614,
     "end_time": "2024-03-28T14:20:00.611368",
     "exception": false,
     "start_time": "2024-03-28T14:20:00.592754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'm creating this object to automatize the assessement.\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Evaluator:\n",
    "    '''\n",
    "        This object handles the evaluation of our models' predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        `model_frost`: MarkovModel\n",
    "            The Markov Model fitted with Robert Forst's stranzas.\n",
    "        `model_poe`: MarkovPoe\n",
    "            The Markov Model fitted with Edgar Allan Poe's stranzas.\n",
    "            \n",
    "        Methods\n",
    "        -------\n",
    "        `predict` Performs the author predictions for a given text.\n",
    "        `f1_score`: Evaluates the f1-score for the predictions of a given set of texts.\n",
    "    '''\n",
    "    def __init__(self, model_frost:MarkovModel, model_poe:MarkovModel):\n",
    "        self.model_frost = model_frost\n",
    "        self.model_poe = model_poe\n",
    "    \n",
    "    def predict(self, text:str)->int:\n",
    "        '''\n",
    "            Predicts a given text's author (0='Robert Frost'; 1='Edgar Allan Poe')\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `text`: str\n",
    "                The strophe which author will be predicted.\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            The predicted author's code.\n",
    "        '''\n",
    "        predictions = [self.model_frost.predict_log_proba_author(text), self.model_poe.predict_log_proba_author(text)]\n",
    "        return np.argmax(predictions)\n",
    "    \n",
    "    def f1_score(self, texts:List[str], targets:List[int])->float:\n",
    "        '''\n",
    "            Measures the f1-score for the predictions of a given batch of texts.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            `texts`: List[str]\n",
    "                The batch of texts.\n",
    "            `targets`: List[int]\n",
    "                The list of targets that will base the f1-score measurement.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The f1-score.\n",
    "        '''\n",
    "        predictions = list(map(self.predict, texts))\n",
    "        return f1_score(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfcdfbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:20:00.624588Z",
     "iopub.status.busy": "2024-03-28T14:20:00.623885Z",
     "iopub.status.idle": "2024-03-28T14:20:01.904887Z",
     "shell.execute_reply": "2024-03-28T14:20:01.903551Z"
    },
    "papermill": {
     "duration": 1.289873,
     "end_time": "2024-03-28T14:20:01.907126",
     "exception": false,
     "start_time": "2024-03-28T14:20:00.617253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training performance. Note the models have overfitted the training sets!\n",
    "Evaluator(model_frost, model_poe).f1_score(train_frost+train_poe, [0 for _ in train_frost]+[1 for _ in train_poe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aecba9ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-28T14:20:01.921585Z",
     "iopub.status.busy": "2024-03-28T14:20:01.921173Z",
     "iopub.status.idle": "2024-03-28T14:20:02.177909Z",
     "shell.execute_reply": "2024-03-28T14:20:02.176795Z"
    },
    "papermill": {
     "duration": 0.266848,
     "end_time": "2024-03-28T14:20:02.180374",
     "exception": false,
     "start_time": "2024-03-28T14:20:01.913526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5797101449275363"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The score drops down substantially with the test set!\n",
    "Evaluator(model_frost, model_poe).f1_score(test_frost+test_poe, [0 for _ in test_frost]+[1 for _ in test_poe])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491cf0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T17:25:44.810198Z",
     "iopub.status.busy": "2024-03-18T17:25:44.809340Z",
     "iopub.status.idle": "2024-03-18T17:25:44.816991Z",
     "shell.execute_reply": "2024-03-18T17:25:44.815925Z",
     "shell.execute_reply.started": "2024-03-18T17:25:44.810156Z"
    },
    "papermill": {
     "duration": 0.006607,
     "end_time": "2024-03-28T14:20:02.193486",
     "exception": false,
     "start_time": "2024-03-28T14:20:02.186879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'>Documentar a nossa `Evaluator` e revisar o projeto! Terminando isso, podemos voltar ao curso</p>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4623586,
     "sourceId": 7878148,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.886054,
   "end_time": "2024-03-28T14:20:02.819916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-28T14:19:53.933862",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
