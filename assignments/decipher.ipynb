{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1ba1b2",
   "metadata": {
    "papermill": {
     "duration": 0.005649,
     "end_time": "2024-05-07T17:33:14.366560",
     "exception": false,
     "start_time": "2024-05-07T17:33:14.360911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cipher Decription\n",
    "* This project is aimed at producing a decipher that discovers the correct mapping &lt;code_letter&gt;:&lt;real_letter&gt; from secret messages.\n",
    "* The solution is based on the concept of Genetic Algorithms. Each decipher is basically a dictionary that represents a hypothetical mapping of letters.\n",
    "* Each offspring will inherit some of the mappings of their parent's dictionary, but with a slight modification/mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c66c850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T17:33:14.377888Z",
     "iopub.status.busy": "2024-05-07T17:33:14.377386Z",
     "iopub.status.idle": "2024-05-07T17:33:14.534150Z",
     "shell.execute_reply": "2024-05-07T17:33:14.533306Z"
    },
    "papermill": {
     "duration": 0.165064,
     "end_time": "2024-05-07T17:33:14.537026",
     "exception": false,
     "start_time": "2024-05-07T17:33:14.371962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Call me Ishmael. Some years ago—never mind how long precisely—having\\nlittle or no money in my purse, and nothing particular to interest me\\non shore, I thought I would sail about a little and see the watery part\\nof the world. It is a way I have of driving off the spleen and\\nregulating the circulation. Whenever I find myself growing grim about\\nthe mouth; whenever it is a damp, drizzly November in my soul; whenever\\nI find myself involuntarily pausing before coffin warehouses, and\\nbringing up the rear of every funeral I meet; and especially whenever\\nmy hypos get such an upper hand of me, that it requires a strong moral\\nprinciple to prevent me from deliberately stepping into the street, and\\nmethodically knocking people’s hats off—then, I account it high time to\\nget to sea as soon as I can. This is my substitute for pistol and ball.\\nWith a philosophical flourish Cato throws himself upon his sword; I\\nquietly take to the ship. There is nothing surprising in this. If they\\nbut knew it, almost all men in their degree, some time or other,\\ncherish very nearly the same feelings towards the ocean with me.',\n",
       " 'There now is your insular city of the Manhattoes, belted round by\\nwharves as Indian isles by coral reefs—commerce surrounds it with her\\nsurf. Right and left, the streets take you waterward. Its extreme\\ndowntown is the battery, where that noble mole is washed by waves, and\\ncooled by breezes, which a few hours previous were out of sight of\\nland. Look at the crowds of water-gazers there.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from re import sub\n",
    "from typing import List\n",
    "\n",
    "def load_text(filename:str)->List[str]:\n",
    "    '''\n",
    "        Reads the .txt file.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        `filename`: str\n",
    "            The name of the poems file.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A list containing each strophe's content.\n",
    "    '''\n",
    "    with open(f'/kaggle/input/cipher/{filename}', 'r') as f:\n",
    "        parag_delim = '\\n\\n'\n",
    "        text = sub('(\\ufeff|\\n)?CHAPTER \\d+\\.[\\w\\'`,;\\. \\-]*', '', f.read()).split(parag_delim)\n",
    "        return text\n",
    "    \n",
    "corpus_model = load_text('moby_dick.txt')\n",
    "corpus_model[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953a6593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T17:33:14.546541Z",
     "iopub.status.busy": "2024-05-07T17:33:14.545818Z",
     "iopub.status.idle": "2024-05-07T17:33:14.556268Z",
     "shell.execute_reply": "2024-05-07T17:33:14.555005Z"
    },
    "papermill": {
     "duration": 0.018181,
     "end_time": "2024-05-07T17:33:14.558999",
     "exception": false,
     "start_time": "2024-05-07T17:33:14.540818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call me Ishmael. Some years ago—never mind how long precisely—having\\nlittle or no money in my purse, and nothing particular to interest me\\non shore, I thought I would sail about a little and see the watery part\\nof the world. It is a way I have of driving off the spleen and\\nregulating the circulation. Whenever I find myself growing grim about\\nthe mouth; whenever it is a damp, drizzly November in my soul; whenever\\nI find myself involuntarily pausing before coffin warehouses, and\\nbringing up the rear of every funeral I meet; and especially whenever\\nmy hypos get such an upper hand of me, that it requires a strong moral\\nprinciple to prevent me from deliberately stepping into the street, and\\nmethodically knocking people’s hats off—then, I account it high time to\\nget to sea as soon as I can. This is my substitute for pistol and ball.\\nWith a philosophical flourish Cato throws himself upon his sword; I\\nquietly take to the ship. There is nothing surprising in this. If they\\nbut knew it, almost all men in their degree, some time or other,\\ncherish very nearly the same feelings towards the ocean with me.',\n",
       " 'There now is your insular city of the Manhattoes, belted round by\\nwharves as Indian isles by coral reefs—commerce surrounds it with her\\nsurf. Right and left, the streets take you waterward. Its extreme\\ndowntown is the battery, where that noble mole is washed by waves, and\\ncooled by breezes, which a few hours previous were out of sight of\\nland. Look at the crowds of water-gazers there.',\n",
       " 'Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\\nHook to Coenties Slip, and from thence, by Whitehall, northward. What\\ndo you see?—Posted like silent sentinels all around the town, stand\\nthousands upon thousands of mortal men fixed in ocean reveries. Some\\nleaning against the spiles; some seated upon the pier-heads; some\\nlooking over the bulwarks of ships from China; some high aloft in the\\nrigging, as if striving to get a still better seaward peep. But these\\nare all landsmen; of week days pent up in lath and plaster—tied to\\ncounters, nailed to benches, clinched to desks. How then is this? Are\\nthe green fields gone? What do they here?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def filter_empty_strings(corpus:List[str])->List[str]:\n",
    "    '''\n",
    "        Some of the treatments made might have generated empty strings. Thus, we need to remove them to guarantee\n",
    "        the success of our model's training.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        `corpus`: List[str]\n",
    "            Our project's corpus.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Our corpus with no empty strings.\n",
    "    '''\n",
    "    _lambda = lambda s: len(s)>0\n",
    "    corpus = list(filter(_lambda, corpus))\n",
    "    return corpus\n",
    "\n",
    "corpus_model = filter_empty_strings(corpus_model)\n",
    "corpus_model[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b54f0ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T17:33:14.569039Z",
     "iopub.status.busy": "2024-05-07T17:33:14.568667Z",
     "iopub.status.idle": "2024-05-07T17:33:14.713574Z",
     "shell.execute_reply": "2024-05-07T17:33:14.712507Z"
    },
    "papermill": {
     "duration": 0.152829,
     "end_time": "2024-05-07T17:33:14.716160",
     "exception": false,
     "start_time": "2024-05-07T17:33:14.563331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from typing import List\n",
    "\n",
    "def remove_punctuation(s:str)->str:\n",
    "    '''\n",
    "        Removes punctuation from a string. Also, already normalizes (lower) and removes any whitespaces\n",
    "        from the string's borders.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        s: `str`\n",
    "            The provided string.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The treated string.\n",
    "    '''\n",
    "        \n",
    "    translation_table = str.maketrans('', '', string.punctuation+ '—“”‘‘’\\n')\n",
    "    return s.lower().strip().translate(translation_table)\n",
    "\n",
    "def treat(s:str)->str:\n",
    "    '''\n",
    "        Applies all transformations mentioned above in a text.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        s: `str`\n",
    "            The provided string.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The treated string.\n",
    "    '''\n",
    "    s = remove_punctuation(s)\n",
    "    s = s.replace(' ','') # Note that I'm disregarding spaces since passwords commonly have it.\n",
    "    return s\n",
    "\n",
    "corpus_model = list(map(treat, corpus_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7a3623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T17:33:14.726550Z",
     "iopub.status.busy": "2024-05-07T17:33:14.725683Z",
     "iopub.status.idle": "2024-05-07T17:33:14.738635Z",
     "shell.execute_reply": "2024-05-07T17:33:14.737347Z"
    },
    "papermill": {
     "duration": 0.02052,
     "end_time": "2024-05-07T17:33:14.741113",
     "exception": false,
     "start_time": "2024-05-07T17:33:14.720593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from re import fullmatch\n",
    "\n",
    "def keep_alpha(corpus:List[str])->List[str]:\n",
    "    '''\n",
    "        For the project's purpose, keep only fragments  that contain letters.\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        `corpus`: List[str]\n",
    "            The project's corpus.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        The filtered corpus.\n",
    "    '''\n",
    "    _lambda = lambda s: fullmatch('[A-z]+', s)\n",
    "    return list(filter(_lambda, corpus))\n",
    "\n",
    "corpus_model = keep_alpha(corpus_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb56324",
   "metadata": {
    "papermill": {
     "duration": 0.003529,
     "end_time": "2024-05-07T17:33:14.748596",
     "exception": false,
     "start_time": "2024-05-07T17:33:14.745067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluator Setting\n",
    "* Since comparing our keys and values to the actual mapping is unadequate for a real world application, we'll instead use a Markov Model. It is going to receive the Moby Dick's corpus and will learn the most common unigrams and bigrams of the English Language.\n",
    "* Therefore, ẃe'll expect that our best decipher is going to create the sequences with higher probability, according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7470970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T17:33:14.758393Z",
     "iopub.status.busy": "2024-05-07T17:33:14.757990Z",
     "iopub.status.idle": "2024-05-07T17:33:17.929893Z",
     "shell.execute_reply": "2024-05-07T17:33:17.928321Z"
    },
    "papermill": {
     "duration": 3.180497,
     "end_time": "2024-05-07T17:33:17.932829",
     "exception": false,
     "start_time": "2024-05-07T17:33:14.752332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Tuple\n",
    "\n",
    "class MarkovModel:\n",
    "    '''\n",
    "       Markov Model, with Add-Epsilon Smoothing.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        `corpus`: List[str]\n",
    "            List with the documents to be used.\n",
    "        `epsilon`: float\n",
    "            Smoothing degree of the probabilities.\n",
    "        `name`: str\n",
    "            A name for your model.\n",
    "            \n",
    "        Methods\n",
    "        ------\n",
    "        `fit`: Generates the model's A and pi.\n",
    "        `predict_log_proba`: Estimates the probability's log of a given sequence.\n",
    "        \n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        `a`: `pd.DataFrame`\n",
    "            The model's A matrix.\n",
    "        `pi`: `pd.Series`\n",
    "            The model's pi vector.\n",
    "        `_vocab`: Set[str]\n",
    "            A set object with all the corpus's vocabulary.\n",
    "    '''\n",
    "    def __init__(self, corpus:List[str], epsilon:float, name:str):\n",
    "        self.corpus = corpus\n",
    "        self.corpus_length = len(self.corpus)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def __vocab(self)->None:\n",
    "        '''\n",
    "            Extraction of all the corpus tokens.\n",
    "            \n",
    "            We create a set with all training tokens and another one disregarding the ones only used as first word of the strophes.\n",
    "        '''\n",
    "        self._vocab, self._a_vocab = [], []\n",
    "        \n",
    "        for doc in self.corpus:\n",
    "            self._vocab += doc\n",
    "            self._a_vocab+=doc[1:] # Not including the first tokens.\n",
    "            \n",
    "        self._vocab, self._a_vocab = set(self._vocab), set(self._a_vocab)        \n",
    "    \n",
    "    def __check_pi(self, token:str)->str:\n",
    "        '''\n",
    "            Masks a sentence's first token with '<UNKNOWN>' mark if it is not included in the training set.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `token`: str\n",
    "                The sentence's first token under scrutiny\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The treated token.\n",
    "        '''\n",
    "        return token if token in self.pi else '<UNKNOWN>'\n",
    "    \n",
    "    def __check_a(self, token1:str, token2:str)->Tuple[str]:\n",
    "        '''\n",
    "            When querying the model's A matrix, checks whether the provided initial and target states are present. If not,\n",
    "            the tokens are masked with the flag '<UNKNOWN>'.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            `token1`: str\n",
    "                The initial state.\n",
    "            `token2`: str\n",
    "                The target state.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The treated tokens inside a tuple.\n",
    "        '''\n",
    "        token1 = token1 if token1 in self.a.index else '<UNKNOWN>'\n",
    "        token2 = token2 if token2 in self.a.columns else '<UNKNOWN>'\n",
    "        return token1, token2\n",
    "    \n",
    "    @property\n",
    "    def pi(self):\n",
    "        '''\n",
    "            Encharged for measuring the model's pi vector.\n",
    "        '''\n",
    "        pi = {}\n",
    "        m = self.a.shape[0]\n",
    "        \n",
    "        for doc in self.corpus:\n",
    "            i = doc[0]\n",
    "            if i not in pi.keys():\n",
    "                pi[i] = 1\n",
    "            else:\n",
    "                pi[i]+=1\n",
    "        \n",
    "        pi['<UNKNOWN>'] = 0 # Defining a key for possible tokens of the test set that were unseen during training.\n",
    "        pi =  (pd.Series(pi)+self.epsilon) / (self.corpus_length+self.epsilon*m)\n",
    "        return pi\n",
    "        \n",
    "    @property\n",
    "    def a(self):\n",
    "        '''\n",
    "            Measures the model's A matrix.\n",
    "        '''\n",
    "        a = {j:{} for j in self._a_vocab}\n",
    "        for doc in self.corpus:\n",
    "            for idx, j in enumerate(doc[1:], start=1):\n",
    "                d_j = a[j]\n",
    "                i = doc[idx-1]\n",
    "                if i not in d_j.keys():\n",
    "                    d_j[i] = 1\n",
    "                else:\n",
    "                    d_j[i] += 1\n",
    "        a['<UNKNOWN>'] = {'<UNKNOWN>':0}\n",
    "        a = pd.DataFrame(a).fillna(0)\n",
    "        \n",
    "        # Essential elements for our Transition Matrix's computing.\n",
    "        m = a.shape[0]\n",
    "        num = (a+self.epsilon)\n",
    "        denom = a.sum(axis=1, skipna=True)+m*self.epsilon\n",
    "        \n",
    "        # Computing the matrix.\n",
    "        a =  num.div(denom, axis=0) \n",
    "        return a\n",
    "\n",
    "    def fit(self):\n",
    "        '''\n",
    "            Fits the algorithm to the provided corpus.\n",
    "        '''\n",
    "        self.__vocab()\n",
    "        self.a\n",
    "        self.pi\n",
    "        return self\n",
    "    \n",
    "    def predict_log_proba(self, text:str)->float:\n",
    "        '''\n",
    "            Estimates the probability's log of a given sequence.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `text`: str\n",
    "                The text whose probability needs to be computed.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The sequence's log probability.\n",
    "        '''\n",
    "        #text = word_tokenize(text.lower())\n",
    "        proba_pi = np.log(self.pi[self.__check_pi(text[0])])\n",
    "        proba_a = np.log([self.a.loc[self.__check_a(text[i-1], text[i])] for i in range(1, len(text))]) \n",
    "        return proba_pi + np.sum(proba_a)                                                   \n",
    "    \n",
    "    def predict_proba(self, text:str)->float:\n",
    "        '''\n",
    "            Estimates the probability of a given sequence.\n",
    "            \n",
    "            *Note:* There is a risk of the output to be 0 for long sequences.\n",
    "            \n",
    "            Parameter\n",
    "            ---------\n",
    "            `text`: str\n",
    "                The text whose probability needs to be computed.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            The sequence's probability.\n",
    "        '''\n",
    "        return np.exp(self.predict_log_proba(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60503cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T17:33:17.943586Z",
     "iopub.status.busy": "2024-05-07T17:33:17.942394Z",
     "iopub.status.idle": "2024-05-07T17:33:18.651514Z",
     "shell.execute_reply": "2024-05-07T17:33:18.650122Z"
    },
    "papermill": {
     "duration": 0.716985,
     "end_time": "2024-05-07T17:33:18.653970",
     "exception": false,
     "start_time": "2024-05-07T17:33:17.936985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting our project's evaluator. \n",
    "model = MarkovModel(corpus_model, 1, 'hi').fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f51ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T17:33:18.663402Z",
     "iopub.status.busy": "2024-05-07T17:33:18.662982Z",
     "iopub.status.idle": "2024-05-07T17:33:23.955205Z",
     "shell.execute_reply": "2024-05-07T17:33:23.953654Z"
    },
    "papermill": {
     "duration": 5.300176,
     "end_time": "2024-05-07T17:33:23.958012",
     "exception": false,
     "start_time": "2024-05-07T17:33:18.657836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.886441306731015"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_log_proba('haasai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f203aa5e",
   "metadata": {
    "papermill": {
     "duration": 0.004512,
     "end_time": "2024-05-07T17:33:23.966912",
     "exception": false,
     "start_time": "2024-05-07T17:33:23.962400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Arrumar uma amostra de textos para os algoritmos genéticos</p>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4913847,
     "sourceId": 8275290,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.053346,
   "end_time": "2024-05-07T17:33:24.592681",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-07T17:33:11.539335",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
