{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86eb3220-32b5-4e8e-afc5-9c58a2e06b22",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Vector Models and Text Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d9c4c-6bb1-4711-8fa6-85662f3d20f9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Basic Definitions for NLP</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Nessa aula, aprenderemos os principais conceitos da área de NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2219a4-7a25-42d7-aab0-6bb08badb4e9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Tokens</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Tokens são entendidos como palavras, pedaços de palavras ou sinais de pontuação. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044f582-b9c9-4bd6-8f75-68a995358d41",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sentences</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Os Cientistas de Dados entendem sentenças como apenas sequências de palavras e sinais de pontuação. \n",
    "        </li>\n",
    "        <li>\n",
    "            Tecnicamente, diríamos que frases são sequências de tokens.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976536c-ced0-4996-9edb-9c4c2c1975ee",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Letters and Characters</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Caracteres consistes em símbolos que podem ser tanto letras, quanto sinais de pontuação ou whitespaces.\n",
    "        </li>\n",
    "        <li>\n",
    "            Modelos de NLP podem tanto ser montados com base em palavras, quanto em caracteres.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c5f18-443b-464a-aece-7da2fca11165",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Corpus</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É o conjunto de textos e documentos de nosso projeto; basicamente o nosso dataset.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a713e33-64c2-4a3f-b4e8-bde6fe5684a3",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Vocabulary</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É o conjunto de todas as palavras/tokens de nosso corpus!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ee9c7-c090-4b06-986e-9cf029f2f053",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> N-Gram</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            N-Gram's são o conjunto de n tokens consecutivos. 1-gram's podem ser chamados de unigrams; 2-gram's, bigrams; e assim por diante.\n",
    "        </li>\n",
    "        <li>\n",
    "            Os n-gram's embasam uma série de algoritmos de NLP, como o word2vec e os modelos de Markov.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='../img/03_ngrams.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d29ffa-1030-4cc8-91d0-5d97de0ca319",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> What is a Vector?</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No contexto de Machine Learning, podemos entender um vetor como uma coleção de números.\n",
    "        </li>\n",
    "        <li>\n",
    "            No contexto de NLP, buscamos vetorizar sentenças e tokens em vetores a fim de usar algoritmos matemáticos.  \n",
    "        </li>\n",
    "        <li> \n",
    "            Com vetores em mãos, podemos simplesmente treinar um modelo que capture os padrões que desejamos. Isso é muito mais eficiente do que codar um programa de RegEx que tente fazer a mesma coisa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a6bf-851a-4cc7-96e4-d7881f5283d8",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bag of Words</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Bag of Words consiste em darmos uma forma numérica ao vocabulário do corpus, sem considerar a ordem das palavras nos textos. \n",
    "        </li>\n",
    "        <li>\n",
    "            Esse é um dos tratamentos de texto mais rudimentares e limitados do NLP, mas ainda assim é bastante utilizado na área.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1326a39-114e-4453-8901-95e6ab0047bd",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Count Vectorizer</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Count Vectorizer é uma técnica de Bag of Words em que montamos uma tabela em que cada linha representa um documento (texto do corpus), e cada coluna um token de todo o vocabulário tido.\n",
    "        </li>\n",
    "        <li>\n",
    "            O valor de cada célula é a contagem de aparições do token no dado documento.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='../img/03_countvec.jpeg'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae50e64-483d-49ed-93d7-104615db4e62",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Implementação Count Vectorizer</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como o vocabulário do Corpus costuma ter uma quantidade enorme de tokens, armazenar as contagens em matrizes convencionais pode ser danoso à memória do seu PC.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por isso, prefira recorrer às matrizes esparsas do Scipy. O objeto CountVectorizer do scikit-learn nos retorna esse tipo de matriz.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10305b6-2efc-472f-959f-d84b158cdcec",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Normalization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como podemos ter documentos maiores e menores em nosso Corpus, normalizar os valores dos vetores é essencial em tarefas de NLP. Afinal, um documento pode ter contagens maiores apenas porque é um texto maior\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ce18d-b155-4838-8b6d-dffe9d08cc2d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Tokenization</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em Machine Learning, tokenization tem um sentido mais vago do que simplemente separar as palavras de uma frase com base em um \\s. Existem várias modalidades de separação de termos com seus respectivos prós e contras. \n",
    "        </li>\n",
    "        <li>\n",
    "            Aqui, apresentaremos os principais aspectos das línguas que devem ser levados em conta no processo de tokenização. A maior parte deles podem ser tratados com as classes do próprio scikit-learn.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65843ac1-3abd-49fb-85ea-38e721c64677",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Considerações ao Tokenizar</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214752b0-a6b6-47f1-8b20-869d56aa0f6e",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'>Punctuation</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em textos, sinais de pontuação aparecem aglutinados às palavras que os antecederam. Levando isso em conta, caso nós não desvinculemos esses sinais das palavras, podemos gerar uma enorme quantidade de tokens.\n",
    "        </li>\n",
    "        <li>\n",
    "            Fazendo a tokenização por \\s em \"Amo gato.\", \"Cadê meu gato?\" e \"O gato morreu\", criamos 3 tokens relacionados à palavra \"gato\" (\"gato.\", \"gato?\" e \"gato\"). Portanto, repare que essa abordagem de tokenização expande bastante a dimensionalidade do dataset.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por outro lado, caso desvinculemos esses sinais, eles próprios passam a ser tokens (\".\", \"?\").  \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6b64e-c320-495a-96c3-7d31541e4926",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'>Casing</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Outro aspecto que devemos considerar na tokenização é se as palavras possuem significado igual, mesmo em caixas diferentes (ex: \"casa branca\", \"Casa Branca\").\n",
    "        </li>\n",
    "        <li>\n",
    "            Caso desejarmos tratar todas as versões das palavras como de mesmo significado, considere torná-las todas caixa-baixa (\"casa branca\").\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040bc99-c85b-4ac8-b9eb-571a85b646da",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'>Accents</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Devemos ter o mesmo tipo de reflexão com as acentuações das palavras. Normalmente, os Cientistas desconsideram esses sinais.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23702017-8a97-47c3-a0c0-bce45b2276b6",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Estratégias de Tokenização</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Cada abordagem de tokenização apresenta seus prós e contras. Tudo dependerá da natureza de seu problema e limitações de hardware.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc420c50-1592-4175-be2d-d40675a37fdc",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Word-based Tokenization</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            É a maneira mais tradicional de tokenização. Sua vantagem sobre os demais métodos é que, mantendo as palavras, conseguimos preservar boa parte da semântica dos textos. \n",
    "        </li>\n",
    "        <li>\n",
    "            Sua desvantagem é o enorme ganho de dimensionalidade gerado. No caso de uma Bag of Words, a matriz deverá conter uma coluna para cada palavra do vocabulário do corpus.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfff11a-361c-424e-b701-df840fef70ab",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Character-based Tokenization</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Essa abordagem é a menos onerosa à memória do computador. No entanto, a semântica dos documentos é substancialmente perdida.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e9dd1-ba91-42ca-8dff-ae429cf82803",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Subword-based Tokenization</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Método de tokenização bastante popular em Deep Learning (Transformers). Consiste em criar tokens com base em fragmentos das palavras. \n",
    "        </li>\n",
    "        <li>\n",
    "            No caso da palavra \"walk\", as suas variantes \"walks\", \"walking\" e \"walked\" poderiam criar os tokens (\"walk\", \"s\", \"ing\", \"ed\").\n",
    "        </li>\n",
    "        <li>\n",
    "            Sua vantagem sobre o Word Tokenization é que esse não considera essas derivações como de sentido igual a \"walk\". Portanto, a interpretação dos documentos pelo modelo pode até ser facilitada com essa abordagem.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "613a41ca-1773-426a-b24f-38d89bf6da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um pequeno tokenizador de palavras caseiro.\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "class WordTokenizer:\n",
    "    '''\n",
    "        Tokenizador de palavras.\n",
    "\n",
    "        Parâmetro\n",
    "        ---------\n",
    "        `sentences`: List[str]\n",
    "            Lista de strings\n",
    "\n",
    "        Função\n",
    "        -------\n",
    "        `tokenize`: Realiza a tokenização de `WordTokenizer.sentences`\n",
    "    '''\n",
    "    def __init__(self, sentences:List[str]):\n",
    "        self.sentences = sentences\n",
    "        self.bag = {}\n",
    "        \n",
    "    def tokenize(self)->pd.DataFrame:\n",
    "        '''\n",
    "            Realiza a tokenização de `WordTokenizer.sentences`\n",
    "\n",
    "            Retorna\n",
    "            -------\n",
    "            `pd.DataFrame` com a contagem de palavras por documento.\n",
    "        '''\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            self.bag[i] = {}\n",
    "            dict_sentence = self.bag[i]\n",
    "            for word in sentence.split():\n",
    "                if word not in dict_sentence.keys():\n",
    "                    dict_sentence[word]=1\n",
    "                else:\n",
    "                    dict_sentence[word]+=1\n",
    "        return pd.DataFrame(self.bag).T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81742e1b-0353-497e-a5cd-0cecf65f5f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>love</th>\n",
       "      <th>cars</th>\n",
       "      <th>hate</th>\n",
       "      <th>her</th>\n",
       "      <th>becase</th>\n",
       "      <th>don't</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     I  love  cars  hate  her  becase  don't  like\n",
       "0  1.0   1.0   1.0   0.0  0.0     0.0    0.0   0.0\n",
       "1  2.0   0.0   0.0   1.0  2.0     1.0    1.0   1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordTokenizer(['I love cars', 'I hate her becase I don\\'t like her']).tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd51ce-3578-4cf8-bb27-5f17c2c864ac",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Stopwords</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Stopwords são palavras de pouco valor para a nossa tarefa de NLP, e que têm uma presença recorrente nos textos de maneira geral.\n",
    "        </li>\n",
    "        <li>\n",
    "            Normalmente, stopwords costumam ser preposições, conjunções e mais alguns verbos comuns. \n",
    "        </li>\n",
    "        <li>\n",
    "            As classes do scikit-learn costumam apenas tratar stopwords da língua inglesa. Caso esteja trabalhando com textos de outra língua, você pode recorrer ao NLTK. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47502a37-239b-435e-a35f-6d686afe3ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo os stopwords da língua portuguesa.\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('portuguese')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b874ec-d1a3-47fe-be04-2d4282cc8eb2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Stemming and Lemmatization</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Stemming e Lemmatization são técnicas de recorte de palavras. Com elas, tentamos remover prefixos e sufixos de verbos, substantivos a fim de mantermos apenas a raiz, que representa o significado central do termo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60849e-c753-4438-8da2-e65c946ae388",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Stemming</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            É a estratégia mais rudimentar de preservação da raiz do termo, baseada em pura heurística.\n",
    "        </li>\n",
    "        <li>\n",
    "            Existem inúmeras estratégias de stemming, cada uma com as suas virtudes. Uma abordagem bastante comum é a de Martin Porter, contida no NLTK.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "634d7348-79a9-4dff-a3d7-0ddc4b46560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bosses and beach'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem('bosses and beaches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b40b5-3546-46a9-acb0-07fa0bb344e5",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Lemmatization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            A Lematização busca converter palavras derivadas à sua respectiva palavra primitiva.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por exemplo, \"Was\" é lematizado como \"Be\", e \"Mice\" se torna \"Mouse\".\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa abordagem é muito mais poderosa do que stemming, pois consegue atuar em substantivos ou verbos irregulares, por exemplo (\"Was\"->\"Be\").\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b185343-b2ad-490a-b3e7-07f0e8c6dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 'growing' como verbo:  grow\n",
      "Lematizando 'growing' como adjetivo:  growing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Invocando o Lematizador.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A função de lematização da classe possui o argumento `pos`. Ele sinalizará a classe gramatical da palavra, podendo influenciar na\n",
    "# transformação.\n",
    "print('Lematizando \\'growing\\' como verbo: ', lemmatizer.lemmatize('growing', pos=wordnet.VERB))\n",
    "print('Lematizando \\'growing\\' como adjetivo: ', lemmatizer.lemmatize('growing', pos=wordnet.ADJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68e946-e781-4dac-ab33-e5c6a6c9c103",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Stemming and Lemmatization Demo</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Quando trabalhamos com corpus gigantescos, é inviável fazermos pos taggings manualmente. Felizmente, podemos recorrer à função `pos_tag`, do nltk, para automatizarmos isso.\n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, as tags retornadas pela função não são compatíveis com as do Lemmatizador. Por isso, vamos ter que programar uma breve função para nos ajudar nessa conversão.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b209780b-b272-4a6d-858b-783830c66861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    from nltk.corpus import wordnet\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3615970-780c-430d-ae53-0d080c2bb5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I be younger so much young than today I never need anybody's help in any way "
     ]
    }
   ],
   "source": [
    "# Agora, lematizando uma sentença com pos tagging automático\n",
    "from nltk import pos_tag\n",
    "sentence = 'When I was younger so much younger than today I never needed anybody\\'s help in any way'.split()\n",
    "\n",
    "for word, pos in pos_tag(sentence):\n",
    "    print(lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c3e59-c6c4-4955-88da-22eac6c74fac",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Count Vectorizer (Code)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Essa aula consiste em um pequeno projeto que engloba o uso do `CountVectorizer`. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a5bd8-ebb1-4379-98e1-4f3f3000a51b",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Vector Similarity</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A comparação de vetores é algo corriqueiro no contexto de NLP. Podemos usar determinadas métricas a fim de encontrarmos vetores cujas sentenças tem semântica parecida com a nossa. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a919bc9-a7ec-4635-9d41-f06e6ca8d19c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Principais Distâncias em NLP </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c37dc2-93e6-407c-bceb-fde0c008867c",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'>  Distâncias Minkowski</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            As Distâncias Minkowski são um critério clássico na comparação de vetores, com um destaque especial à Distância Euclidiana (p=2).\n",
    "            <center style='margin-top:20px'> \n",
    "                $\\displaystyle ||x-y||_{p}=[\\sum_{i=1}^{n}{|x_{i}-y_{i}|^{p}}]^{\\frac{1}{p}}$\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6075d-8e1a-4e0a-9645-e2034b401b85",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Cosine Similarity</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            O cosseno do ângulo entre dois vetores pode ser usado como uma métrica de similaridade entre eles. Quando $\\cos{\\theta}\\to{1}$, $\\theta \\to{0^{\\circ}}$, sinalizando similaridade entre os vetores. Já quando $\\cos{\\theta}\\to{-1}$, $\\theta \\to{180^{\\circ}}$, indicando discrepância.\n",
    "            <center style='margin-top:20px'> \n",
    "                $\\displaystyle \\cos{\\theta}=\\frac{{x}\\cdot{y}}{||x||_{2} ||y||_{2}}$\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f309d2f-70ed-4ed4-82b6-d3aace2491d3",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Cosine Distance</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            É basicamente 1-Cosine Similarity. Seu objetivo é fazer com que vetores parecidos tenham os valores mais baixos, e os mais discrepantes valores maiores.\n",
    "            <center style='margin-top:20px'> \n",
    "                $\\displaystyle \\text{Cosine Distance}=1-\\text{Cosine Similarity}$\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1502e66-4f03-4176-a973-5317da8f3015",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Por que Cosseno é Mais Popular? </h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            O uso das métricas de cosseno se deve ao fato de elas serem menos sensíveis à magnitude de cada componente dos vetores. Já a Distância Euclidiana tem o defeito de aproximar vetores de temas distintos, mas com textos de tamanhos similares. \n",
    "        </li>\n",
    "        <li>\n",
    "            Com isso, textos mais longos podem ser desassociados de textos mais curtos com mesma temática pela Distância Euclidiana. Segue exemplo dado pelo curso.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='../img/03_cossine_euclidian.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0418208-b7da-4fe4-9388-ff14b2f98b09",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> TF-IDF (Theory)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            TF-IDF é uma técnica de numeralização dos documentos, que propõe diminuir a relevância de stopwords automaticamente. \n",
    "        </li>\n",
    "        <li>\n",
    "            Com ele, palavras que aparecem frequentemente em todos os documentos recebem números menores nos vetores dos documentos.\n",
    "        </li>\n",
    "        <li>\n",
    "            Isso não descarta a necessidade de descartarmos preposições e conjunções do corpus, visando reduzir a dimensionalidade de nosso $X$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702747be-b21a-46bf-97dd-354bf12a0f5f",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Fórmula do TF-IDF</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O TF-IDF é composto por dois termos: o Term Frequency (TF) é a contagem de vezes que o token surge no documento. Já a Inverse Document Frequency é o log da frequência inversa de documentos que têm o termo.\n",
    "        </li>\n",
    "        <li>\n",
    "            Seja t um token, d um documento do corpus, N(t) a contagem de documentos com o token e N o número de documentos do corpus, o TF-IDF pode ser definido como:\n",
    "            <center style='margin-top:20px'> \n",
    "                $\\displaystyle tfidf(t,d)=\\text{contagem}(t,d)\\times{\\log{\\frac{N}{N(t)}}}$\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3aa03-de63-4c66-b65e-eb275b85f624",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Why Take the Log?</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Tiramos o $\\log$ da frequência inversa, visando evitar que tokens de presença muito rara recebam números enormes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f14a09b-eaeb-4aad-8466-7d1c4a805356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1067942/2761721555.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  y = np.log(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$y=\\\\log(x)$')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observe como a função log suaviza o quociente N/N(t).\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, 100, 501)\n",
    "y = np.log(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('$y=\\log(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbc368-318f-4980-b4aa-2e7bd28ebb0d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Term Frequency Variations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vale mencionar que existem variações do que seria o TF na equação: \n",
    "            <ul> \n",
    "                <li> \n",
    "                    Binário: 1, caso haja o token. Senão, 0.\n",
    "                </li>\n",
    "                <li>\n",
    "                    Proporção de aparições no documento: $\\displaystyle \\frac{\\text{contagem}(t,d)}{\\sum_{t^{'}\\in{d}}\\text{contagem}(t^{'},d)}$\n",
    "                </li>\n",
    "                <li> \n",
    "                    Logarítmo: $\\log(1+\\text{contagem}(t,d))$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73eae26-881f-4b1f-a198-db365f93c428",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Inverse Document Frequency Variations</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O IDF também recebe algumas variações:\n",
    "            <ul> \n",
    "                <li> \n",
    "                    Smooth IDF: $\\log(\\frac{N}{N(t)+1}+1)$: Evita que o IDF tradicional fique como 0, no caso em que N=N(t). \n",
    "                </li>\n",
    "                <li style='margin-top:20px'>\n",
    "                    IDF Max: $\\displaystyle \\log(\\frac{\\max_{t^{'}\\in{d}}N(t^{'})}{N(t)})$, considera a contagem do token do documento com maior aparecimento no corpus.\n",
    "                </li>\n",
    "                <li style='margin-top:20px'> \n",
    "                    Probabilistic IDF: $\\log(\\frac{N-N(t)}{N(t)})$\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999efc2d-c53e-41bb-a81d-622d670bc92d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Normalizing TF-IDF</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma vez tendo os vetores de TF-IDF, podemos normalizá-los (l1, l2). A classe do scikit-learn é programada para já fazer isso por nós.\n",
    "        </li>\n",
    "        <li>\n",
    "            A vantagem disso é tornar o uso da Distância Euclidiana e Similaridade Cosseno equivalentes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e9a56-94db-4143-a399-24a33e09c25e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Word-to-Index Mapping</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Quando matricializamos nossos documentos, devemos garantir que a contagem/TD-IDF de um token seja posicionada na respectiva coluna deste.\n",
    "        </li>\n",
    "        <li>\n",
    "            Esse mapeamento pode ser realizado por meio de dicionários.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22910701-84ba-4cb8-94db-86c37afdb563",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Tokens Desconhecidos</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É possível que o set de teste do projeto contenha determinados tokens que não estavam no de treino.  \n",
    "        </li>\n",
    "        <li>\n",
    "            Diante dessa situação, podemos tanto ignorar tais palavras, quanto adotar uma abordagem um pouco mais complexa: criamos uma coluna na matriz de contagens voltada a tokens raros, e inserimos os dados de qualquer palavra infrequente a ela.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c1b982b2-2bcd-46db-9544-9c6523562bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54238072, 0.        , 0.        , 0.43789374, 0.71698831],\n",
       "       [0.4407892 , 0.58269162, 0.58269162, 0.35587333, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from sklearn.preprocessing import normalize\n",
    "from typing import List, Tuple, Set\n",
    "\n",
    "class TF_IDF:\n",
    "    @staticmethod\n",
    "    def __tokenize_sentences(texts:List[str])->Tuple[List[List[str]], Set[str]]:\n",
    "        '''\n",
    "            Tokeniza uma lista de sentenças.\n",
    "\n",
    "            Parâmetro\n",
    "            ---------\n",
    "            `texts`: List[str]\n",
    "                Lista com as sentenças.\n",
    "\n",
    "            Retorna\n",
    "            -------\n",
    "            Uma tuplace com uma lista com os tokens, por frase, e outra com um set do vocabulário..\n",
    "        '''\n",
    "        tkns, vocab = [], []\n",
    "        for s in texts:\n",
    "            tokens = word_tokenize(s)\n",
    "            tkns.append(tokens)\n",
    "            vocab.extend(tokens)\n",
    "        return tkns, set(vocab)\n",
    "\n",
    "    def __init__(self, texts:List[str]):\n",
    "        self._tokens, self._vocab = self.__tokenize_sentences(texts)\n",
    "\n",
    "    def __tf(self):\n",
    "        '''\n",
    "            Mensuração dos TF's.\n",
    "        '''\n",
    "        self._tf = {}\n",
    "        for i, list_tokens in enumerate(self._tokens):\n",
    "            self._tf[i] = dict(Counter(list_tokens))\n",
    "        return pd.DataFrame(self._tf).fillna(0).T\n",
    "        \n",
    "    def __idf(self):\n",
    "        '''\n",
    "            Mensuração dos IDF' (smooth), para cada token do vocabulário.\n",
    "        '''\n",
    "        self._idf = {t:0 for t in self._vocab}\n",
    "        for t in self._vocab:\n",
    "            for list_tokens in self._tokens:\n",
    "                if t in list_tokens:\n",
    "                    self._idf[t]+=1\n",
    "                else:\n",
    "                    pass\n",
    "        return np.log(len(self._tokens) / (pd.Series(self._idf)+1)+1)\n",
    "\n",
    "    def transform(self):\n",
    "        '''\n",
    "            Cria a matriz de TF-IDF, normalizada.\n",
    "        '''\n",
    "        tf_idf = (self.__tf() * self.__idf())\n",
    "        return normalize(tf_idf, axis=1, norm='l2')\n",
    "\n",
    "X = ['I like dogs', ' I dogs are awesome', 'dogs dogs']\n",
    "a = TF_IDF(X)\n",
    "a.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f31df-69b7-45d5-bb70-ea5152cad15e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Neural Word Embeddings</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em Deep Learning, existem redes neurais especializadas em vetorização de documentos. Diferentemente dos métodos Bag of Words, cada palavra recebe um vetor, fazendo com que o documento seja representado como uma sequência de vetores!\n",
    "        </li>\n",
    "        <li>\n",
    "            Isso garante que nós tenhamos muito mais informações sobre nossas sentenças.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639fa84-06e8-4aad-b05b-f65180aa9860",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Word2vec Basics</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Na Word2Vec, treinamos uma Rede Neural que prevê, dada uma palavra, quais outras estariam próximas a ela, a uma distância d dela.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por exemplo, na frase \"Felipe anda de carro toda quarta à noite\", se setássemos $d=2$, consideraríamos \"anda\", \"de\", \"toda\" e \"quarta\" como targets de \"carro\". \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8383d4-c7d9-45b0-a422-9c17891fd287",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> GloVe</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            GloVe é uma técnica não baseada em redes neurais, mas usada por algumas delas em treinamento.\n",
    "        </li>\n",
    "        <li>\n",
    "            Em uma frase, conferimos pontuações a palavras, dada a sua distância ao token de input. No caso da frase de exemplo, \"de\" e \"toda\" poderiam receber 1, enquanto \"anda\" e \"quarta\", 1/2. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3f06b-5d97-407a-8087-26abe0c610d6",
   "metadata": {},
   "source": [
    "<p style='color:red'> Expliquei Word2Vec e GloVe. Aula 24 (6:10). </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
