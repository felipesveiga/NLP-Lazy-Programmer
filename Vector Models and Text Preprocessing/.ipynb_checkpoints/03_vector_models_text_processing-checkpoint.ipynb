{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86eb3220-32b5-4e8e-afc5-9c58a2e06b22",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Vector Models and Text Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d9c4c-6bb1-4711-8fa6-85662f3d20f9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Basic Definitions for NLP</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Nessa aula, aprenderemos os principais conceitos da área de NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2219a4-7a25-42d7-aab0-6bb08badb4e9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Tokens</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Tokens são entendidos como palavras, pedaços de palavras ou sinais de pontuação. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044f582-b9c9-4bd6-8f75-68a995358d41",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sentences</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Os Cientistas de Dados entendem sentenças como apenas sequências de palavras e sinais de pontuação. \n",
    "        </li>\n",
    "        <li>\n",
    "            Tecnicamente, diríamos que frases são sequências de tokens.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976536c-ced0-4996-9edb-9c4c2c1975ee",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Letters and Characters</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Caracteres consistes em símbolos que podem ser tanto letras, quanto sinais de pontuação ou whitespaces.\n",
    "        </li>\n",
    "        <li>\n",
    "            Modelos de NLP podem tanto ser montados com base em palavras, quanto em caracteres.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c5f18-443b-464a-aece-7da2fca11165",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Corpus</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É o conjunto de textos e documentos de nosso projeto; basicamente o nosso dataset.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a713e33-64c2-4a3f-b4e8-bde6fe5684a3",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Vocabulary</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É o conjunto de todas as palavras/tokens de nosso corpus!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ee9c7-c090-4b06-986e-9cf029f2f053",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> N-Gram</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            N-Gram's são o conjunto de n tokens consecutivos. 1-gram's podem ser chamados de unigrams; 2-gram's, bigrams; e assim por diante.\n",
    "        </li>\n",
    "        <li>\n",
    "            Os n-gram's embasam uma série de algoritmos de NLP, como o word2vec e os modelos de Markov.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='../img/03_ngrams.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d29ffa-1030-4cc8-91d0-5d97de0ca319",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> What is a Vector?</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No contexto de Machine Learning, podemos entender um vetor como uma coleção de números.\n",
    "        </li>\n",
    "        <li>\n",
    "            No contexto de NLP, buscamos vetorizar sentenças e tokens em vetores a fim de usar algoritmos matemáticos.  \n",
    "        </li>\n",
    "        <li> \n",
    "            Com vetores em mãos, podemos simplesmente treinar um modelo que capture os padrões que desejamos. Isso é muito mais eficiente do que codar um programa de RegEx que tente fazer a mesma coisa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a6bf-851a-4cc7-96e4-d7881f5283d8",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bag of Words</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Bag of Words consiste em darmos uma forma numérica ao vocabulário do corpus, sem considerar a ordem das palavras nos textos. \n",
    "        </li>\n",
    "        <li>\n",
    "            Esse é um dos tratamentos de texto mais rudimentares e limitados do NLP, mas ainda assim é bastante utilizado na área.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1326a39-114e-4453-8901-95e6ab0047bd",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Count Vectorizer</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Count Vectorizer é uma técnica de Bag of Words em que montamos uma tabela em que cada linha representa um documento (texto do corpus), e cada coluna um token de todo o vocabulário tido.\n",
    "        </li>\n",
    "        <li>\n",
    "            O valor de cada célula é a contagem de aparições do token no dado documento.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='../img/03_countvec.jpeg'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae50e64-483d-49ed-93d7-104615db4e62",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Implementação Count Vectorizer</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como o vocabulário do Corpus costuma ter uma quantidade enorme de tokens, armazenar as contagens em matrizes convencionais pode ser danoso à memória do seu PC.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por isso, prefira recorrer às matrizes esparsas do Scipy. O objeto CountVectorizer do scikit-learn nos retorna esse tipo de matriz.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10305b6-2efc-472f-959f-d84b158cdcec",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Normalization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como podemos ter documentos maiores e menores em nosso Corpus, normalizar os valores dos vetores é essencial em tarefas de NLP. Afinal, um documento pode ter contagens maiores apenas porque é um texto maior\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ce18d-b155-4838-8b6d-dffe9d08cc2d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Tokenization</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em Machine Learning, tokenization tem um sentido mais vago do que simplemente separar as palavras de uma frase com base em um \\s. Existem várias modalidades de separação de termos com seus respectivos prós e contras. \n",
    "        </li>\n",
    "        <li>\n",
    "            Aqui, apresentaremos os principais aspectos das línguas que devem ser levados em conta no processo de tokenização. A maior parte deles podem ser tratados com as classes do próprio scikit-learn.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65843ac1-3abd-49fb-85ea-38e721c64677",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Considerações ao Tokenizar</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214752b0-a6b6-47f1-8b20-869d56aa0f6e",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'>Punctuation</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em textos, sinais de pontuação aparecem aglutinados às palavras que os antecederam. Levando isso em conta, caso nós não desvinculemos esses sinais das palavras, podemos gerar uma enorme quantidade de tokens.\n",
    "        </li>\n",
    "        <li>\n",
    "            Fazendo a tokenização por \\s em \"Amo gato.\", \"Cadê meu gato?\" e \"O gato morreu\", criamos 3 tokens relacionados à palavra \"gato\" (\"gato.\", \"gato?\" e \"gato\"). Portanto, repare que essa abordagem de tokenização expande bastante a dimensionalidade do dataset.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por outro lado, caso desvinculemos esses sinais, eles próprios passam a ser tokens (\".\", \"?\").  \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6b64e-c320-495a-96c3-7d31541e4926",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'>Casing</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Outro aspecto que devemos considerar na tokenização é se as palavras possuem mesmo significado mesmo em caixas diferentes (ex: \"casa branca\", \"Casa Branca\").\n",
    "        </li>\n",
    "        <li>\n",
    "            Caso desejemos tratar todas as versões das palavras como de mesmo significado, considere torná-las todas caixa-baixa (\"casa branca\").\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040bc99-c85b-4ac8-b9eb-571a85b646da",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'>Accents</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Devemos ter o mesmo tipo de reflexão com as acentuações das palavras. Normalmente, os Cientistas desconsideram esses sinais.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23702017-8a97-47c3-a0c0-bce45b2276b6",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Estratégias de Tokenização</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Cada abordagem de tokenização apresenta seus prós e contras. Tudo dependerá da natureza de seu problema e limitações de hardware.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc420c50-1592-4175-be2d-d40675a37fdc",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Word-based Tokenization</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            É a maneira mais tradicional de tokenização. Sua vantagem sobre os demais métodos é que, mantendo as palavras, conseguimos preservar boa parte da semântica dos textos. \n",
    "        </li>\n",
    "        <li>\n",
    "            Sua desvantagem é o enorme ganho de dimensionalidade gerado. No caso de uma Bag of Words, a matriz deverá conter uma coluna para cada palavra do vocabulário do corpus.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfff11a-361c-424e-b701-df840fef70ab",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Character-based Tokenization</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Essa abordagem é a menos onerosa à memória do computador. No entanto, a semântica dos documentos é substancialmente perdida.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e9dd1-ba91-42ca-8dff-ae429cf82803",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Subword-based Tokenization</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Método de tokenização bastante popular em Deep Learning (Transformers). Consiste em criar tokens com base em fragmentos das palavras. \n",
    "        </li>\n",
    "        <li>\n",
    "            No caso da palavra \"walk\", as suas variantes \"walks\", \"walking\" e \"walked\" poderiam criar os tokens (\"walk\", \"s\", \"ing\", \"ed\").\n",
    "        </li>\n",
    "        <li>\n",
    "            Sua vantagem sobre o Word Tokenization é que esse não considera essas derivações como de sentido igual a \"walk\". Portanto, a interpretação dos documentos pelo modelo pode até ser facilitada com essa abordagem.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "613a41ca-1773-426a-b24f-38d89bf6da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um pequeno tokenizador de palavras caseiro.\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "class WordTokenizer:\n",
    "    '''\n",
    "        Tokenizador de palavras.\n",
    "\n",
    "        Parâmetro\n",
    "        ---------\n",
    "        `sentences`: List[str]\n",
    "            Lista de strings\n",
    "\n",
    "        Função\n",
    "        -------\n",
    "        `tokenize`: Realiza a tokenização de `WordTokenizer.sentences`\n",
    "    '''\n",
    "    def __init__(self, sentences:List[str]):\n",
    "        self.sentences = sentences\n",
    "        self.bag = {}\n",
    "        \n",
    "    def tokenize(self)->pd.DataFrame:\n",
    "        '''\n",
    "            Realiza a tokenização de `WordTokenizer.sentences`\n",
    "\n",
    "            Retorna\n",
    "            -------\n",
    "            `pd.DataFrame` com a contagem de palavras por documento.\n",
    "        '''\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            self.bag[i] = {}\n",
    "            dict_sentence = self.bag[i]\n",
    "            for word in sentence.split():\n",
    "                if word not in dict_sentence.keys():\n",
    "                    dict_sentence[word]=1\n",
    "                else:\n",
    "                    dict_sentence[word]+=1\n",
    "        return pd.DataFrame(self.bag).T.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81742e1b-0353-497e-a5cd-0cecf65f5f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>love</th>\n",
       "      <th>cars</th>\n",
       "      <th>hate</th>\n",
       "      <th>her</th>\n",
       "      <th>becase</th>\n",
       "      <th>don't</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     I  love  cars  hate  her  becase  don't  like\n",
       "0  1.0   1.0   1.0   0.0  0.0     0.0    0.0   0.0\n",
       "1  2.0   0.0   0.0   1.0  2.0     1.0    1.0   1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordTokenizer(['I love cars', 'I hate her becase I don\\'t like her']).tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd51ce-3578-4cf8-bb27-5f17c2c864ac",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Stopwords</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Stopwords são palavras de pouco valor para a nossa tarefa de NLP, e que têm uma presença recorrente nos textos de maneira geral.\n",
    "        </li>\n",
    "        <li>\n",
    "            Normalmente, stopwords costumam ser preposições, conjunções e mais alguns verbos comuns. \n",
    "        </li>\n",
    "        <li>\n",
    "            As classes do scikit-learn costumam apenas tratar stopwords da língua inglesa. Caso esteja trabalhando com textos de outra língua, você pode recorrer ao NLTK. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47502a37-239b-435e-a35f-6d686afe3ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo os stopwords da língua portuguesa.\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('portuguese')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b874ec-d1a3-47fe-be04-2d4282cc8eb2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Stemming and Lemmatization</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Stemming e Lemmatization são técnicas de recorte de palavras. Com elas, tentamos remover prefixos e sufixos de verbos, substantivos a fim de mantermos apenas a raiz, que representa o significado central do termo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60849e-c753-4438-8da2-e65c946ae388",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Stemming</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            É a estratégia mais rudimentar de preservação da raiz do termo, baseada em pura heurística.\n",
    "        </li>\n",
    "        <li>\n",
    "            Existem inúmeras estratégias de stemming, cada uma com as suas virtudes. Uma abordagem bastante comum é a de Martin Porter, contida no NLTK.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "634d7348-79a9-4dff-a3d7-0ddc4b46560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bosses and beach'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem('bosses and beaches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b40b5-3546-46a9-acb0-07fa0bb344e5",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Lemmatization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            A Lematização busca converter palavras derivadas à sua respectiva palavra primitiva.\n",
    "        </li>\n",
    "        <li> \n",
    "            Por exemplo, \"Was\" é lematizado como \"Be\", e \"Mice\" se torna \"Mouse\".\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa abordagem é muito mais poderosa do que stemming, pois consegue atuar em substantivos ou verbos irregulares, por exemplo (\"Was\"->\"Be\").\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b185343-b2ad-490a-b3e7-07f0e8c6dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 'growing' como verbo:  grow\n",
      "Lematizando 'growing' como adjetivo:  growing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Invocando o Lematizador.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A função de lematização da classe possui o argumento `pos`. Ele sinalizará a classe gramatical da palavra, podendo influenciar na\n",
    "# transformação.\n",
    "print('Lematizando \\'growing\\' como verbo: ', lemmatizer.lemmatize('growing', pos=wordnet.VERB))\n",
    "print('Lematizando \\'growing\\' como adjetivo: ', lemmatizer.lemmatize('growing', pos=wordnet.ADJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68e946-e781-4dac-ab33-e5c6a6c9c103",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Stemming and Lemmatization Demo</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Quando trabalhamos com corpus gigantescos, é inviável fazermos pos taggings manualmente. Felizmente, podemos recorrer à função `pos_tag`, do nltk, para automatizarmos isso.\n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, as tags retornadas pela função não são compatíveis com as do Lemmatizador. Por isso, vamos ter que programar uma breve função para nos ajudar nessa conversão.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b209780b-b272-4a6d-858b-783830c66861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    from nltk.corpus import wordnet\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3615970-780c-430d-ae53-0d080c2bb5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I be younger so much young than today I never need anybody's help in any way "
     ]
    }
   ],
   "source": [
    "# Agora, lematizando uma sentença com pos tagging automático\n",
    "from nltk import pos_tag\n",
    "sentence = 'When I was younger so much younger than today I never needed anybody\\'s help in any way'.split()\n",
    "\n",
    "for word, pos in pos_tag(sentence):\n",
    "    print(lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c3e59-c6c4-4955-88da-22eac6c74fac",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Count Vectorizer (Code)</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Essa aula consiste em um pequeno projeto que engloba o uso do `CountVectorizer`. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619a80b-f51c-4d18-b08e-995a0be17842",
   "metadata": {},
   "source": [
    "<p style='color:red'> Aula 18 </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
