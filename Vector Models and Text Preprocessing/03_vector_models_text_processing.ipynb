{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86eb3220-32b5-4e8e-afc5-9c58a2e06b22",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Vector Models and Text Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d9c4c-6bb1-4711-8fa6-85662f3d20f9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Basic Definitions for NLP</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Nessa aula, aprenderemos os principais conceitos da área de NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2219a4-7a25-42d7-aab0-6bb08badb4e9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Tokens</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Tokens são entendidos como palavras, pedaços de palavras ou sinais de pontuação. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044f582-b9c9-4bd6-8f75-68a995358d41",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Sentences</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Os Cientistas de Dados entendem sentenças como apenas sequências de palavras e sinais de pontuação. \n",
    "        </li>\n",
    "        <li>\n",
    "            Tecnicamente, diríamos que frases são sequências de tokens.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976536c-ced0-4996-9edb-9c4c2c1975ee",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Letters and Characters</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Caracteres consistes em símbolos que podem ser tanto letras, quanto sinais de pontuação ou whitespaces.\n",
    "        </li>\n",
    "        <li>\n",
    "            Modelos de NLP podem tanto ser montados com base em palavras, quanto em caracteres.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c5f18-443b-464a-aece-7da2fca11165",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Corpus</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É o conjunto de textos e documentos de nosso projeto; basicamente o nosso dataset.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a713e33-64c2-4a3f-b4e8-bde6fe5684a3",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Vocabulary</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É o conjunto de todas as palavras/tokens de nosso corpus!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ee9c7-c090-4b06-986e-9cf029f2f053",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> N-Gram</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            N-Gram's são o conjunto de n tokens consecutivos. 1-gram's podem ser chamados de unigrams; 2-gram's, bigrams; e assim por diante.\n",
    "        </li>\n",
    "        <li>\n",
    "            Os n-gram's embasam uma série de algoritmos de NLP, como o word2vec e os modelos de Markov.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='../img/03_ngrams.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d29ffa-1030-4cc8-91d0-5d97de0ca319",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> What is a Vector?</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            No contexto de Machine Learning, podemos entender um vetor como uma coleção de números.\n",
    "        </li>\n",
    "        <li>\n",
    "            No contexto de NLP, buscamos vetorizar sentenças e tokens em vetores a fim de usar algoritmos matemáticos.  \n",
    "        </li>\n",
    "        <li> \n",
    "            Com vetores em mãos, podemos simplesmente treinar um modelo que capture os padrões que desejamos. Isso é muito mais eficiente do que codar um programa de RegEx que tente fazer a mesma coisa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a6bf-851a-4cc7-96e4-d7881f5283d8",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Bag of Words</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Bag of Words consiste em darmos uma forma numérica ao vocabulário do corpus, sem considerar a ordem das palavras nos textos. \n",
    "        </li>\n",
    "        <li>\n",
    "            Esse é um dos tratamentos de texto mais rudimentares e limitados do NLP, mas ainda assim é bastante utilizado na área.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1326a39-114e-4453-8901-95e6ab0047bd",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Count Vectorizer</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Count Vectorizer é uma técnica de Bag of Words em que montamos uma tabela em que cada linha representa um documento (texto do corpus), e cada coluna um token de todo o vocabulário tido.\n",
    "        </li>\n",
    "        <li>\n",
    "            O valor de cada célula é a contagem de aparições do token no dado documento.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='../img/03_countvec.jpeg'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae50e64-483d-49ed-93d7-104615db4e62",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Implementação Count Vectorizer</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como o vocabulário do Corpus costuma ter uma quantidade enorme de tokens, armazenar as contagens em matrizes convencionais pode ser danoso à memória do seu PC.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por isso, prefira recorrer às matrizes esparsas do Scipy. O objeto CountVectorizer do scikit-learn nos retorna esse tipo de matriz.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10305b6-2efc-472f-959f-d84b158cdcec",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Normalization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como podemos ter documentos maiores e menores em nosso Corpus, normalizar os valores dos vetores é essencial em tarefas de NLP. Afinal, um documento pode ter contagens maiores apenas porque é um texto maior\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ce18d-b155-4838-8b6d-dffe9d08cc2d",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Tokenization</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em Machine Learning, tokenization tem um sentido mais vago do que simplemente separar as palavras de uma frase com base em um \\s. Existem várias modalidades de separação de termos com seus respectivos prós e contras. \n",
    "        </li>\n",
    "        <li>\n",
    "            Aqui, apresentaremos os principais aspectos das línguas que devem ser levados em conta no processo de tokenização.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214752b0-a6b6-47f1-8b20-869d56aa0f6e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Punctuation</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em textos, sinais de pontuação aparecem aglutinados às palavras que os antecederam. Levando isso em conta, caso nós não desvinculemos esses sinais das palavras, podemos gerar uma enorme quantidade de tokens.\n",
    "        </li>\n",
    "        <li>\n",
    "            Fazendo a tokenização por \\s em \"Amo gato.\", \"Cadê meu gato?\" e \"O gato morreu\", criamos 3 tokens relacionados à palavra \"gato\" (\"gato.\", \"gato?\" e \"gato\"). Portanto, repare que essa abordagem de tokenização expande bastante a dimensionalidade do dataset.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por outro lado, caso desvinculemos esses sinais, eles próprios passam a ser tokens (\".\", \"?\").  \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6b64e-c320-495a-96c3-7d31541e4926",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Casing</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Outro aspecto que devemos considerar na tokenização é se as palavras possuem mesmo significado mesmo em caixas diferentes (ex: \"casa branca\", \"Casa Branca\").\n",
    "        </li>\n",
    "        <li>\n",
    "            Caso desejemos tratar todas as versões das palavras como de mesmo significado, considere torná-las todas caixa-baixa (\"casa branca\").\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619a80b-f51c-4d18-b08e-995a0be17842",
   "metadata": {},
   "source": [
    "<p style='color:red'> Aula 13 (6:00)</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
