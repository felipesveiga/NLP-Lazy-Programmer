{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90ad712-b30a-44eb-b112-1fb8f02569bd",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Spam Detection</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Esta seção terá o enfoque de criar um detector de spams, utilizando o Naive Bayes.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac5004-b9da-4fad-961e-5ee3f7f337b9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Naive Bayes Intuition</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Aprenderemos aqui a lógica edo Naive Bayes, inspirado no Teorema de Bayes.\n",
    "            <center style='margin-top:20px'>\n",
    "                $\\displaystyle p(Y|X)=\\frac{p(X|Y)p(Y)}{\\sum_{y}{p(X|y)p(y)}}$\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'>\n",
    "            Em situações em que temos múltiplas features, consideramos $X=(x_i,...,x_{m})$.\n",
    "            <center style='margin-top:20px'>\n",
    "                $\\displaystyle p(Y|X)=\\frac{p(x_i,...,x_{m}|Y)p(Y)}{\\sum_{y}{p(x_i,...,x_{m}|y)p(y)}}$\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500c6d2-1930-4770-ae0e-7d3652a1364c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> The Naive Assumption</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O caráter \"ingênuo\" desse algoritmo está na forma como computamos $p(x_i,...,x_{m}|y)$. Nós a consideraremos como o produto das probabilidades de cada feature $x_i$, condicionada a $y$.\n",
    "            <center style='margin-top:20px'>\n",
    "                $\\displaystyle p(x_i,...,x_{m}|y)=\\prod_{i=1}^{m}{p(x_i|y)}$\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47eafc-0630-483b-89d7-f94d25df3881",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Treinamento</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O treinamento de um Naive Bayes consiste em apenas parametrizar distribuições probabilísticas, com estatísticas das suas features de treinamento.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por exemplo, se assumirmos que nossas features são Gaussianas, criamos uma distribuição para cada uma delas, baseada em seus respectivos $\\overline{x}_{i}$ e $s_{i}$, quando $y=y_i$.\n",
    "        </li>\n",
    "        <li>\n",
    "            Assim, em inferência, computamos $p(x_i|y)$, baseando-nos na PDF dessas distribuições.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33c156-1b43-4fc3-8bef-29769c77738d",
   "metadata": {},
   "source": [
    "<p style='color:red'> Terminei Aula 64; Aula 65</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
